{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroductionToVariationalAutoencoders_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrqeglcAJW3upAuveixMbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VMBoehm/ML_Lectures/blob/main/IntroductionToVariationalAutoencoders_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iquM2NAFde_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Variational AutoEncoders (VAEs) and applications to physical data "
      ],
      "metadata": {
        "id": "Rvu91ZyEFtBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "by Vanessa Boehm (UC Berkeley and LBNL)   \n",
        "Feb 27 2022\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this notebook we will be implementing our first Variational Autoencoder. VAEs are a useful tool for the analysis of physical data because of their probabilistic layout.   \n",
        "\n",
        "We will start from a (non-probalistic) autoencoder and convert it into its variational counterpart.\n",
        "\n",
        "## Autoencoder\n",
        "\n",
        "Recall that an Auto-Encoder consists of two networks: \n",
        "\n",
        "1.   An encoder network that takes the data, $x$, and maps it to a lower-dimensional latent space. We will call this network $f$ and its network parameters $\\phi$.\n",
        "2.   A decoder network that takes the encoded data, $z$, and maps it back to the data space. We will call the result of the reconstruction $x'$, the decoder network $g$ (for generator) and its network parameters $\\psi$.\n",
        "\n",
        "$$ x' = g_\\psi(f_\\phi(x)) \\tag{1}$$\n",
        "\n",
        "An Auto-Encoder is trained to minimize the reconstruction error between the input $x$ and the reconstruction $x'$.\n",
        "\n",
        "$$ \\mathcal{L}_{AE}(\\phi,\\psi) = ||x-g_\\psi(f_\\phi(x))||^2_2 \\tag{2}$$\n",
        "\n",
        "---\n",
        "\n",
        "## Probabilistic Interpretation\n",
        "\n",
        "**In a Variational Auto-Encoder we interpret the reconstruction task  probabilistically:**\n",
        "\n",
        "Compressing the data results in a loss of information about the original data. \n",
        "If we only have access to the compressed data, we have no chance of knowing what the original data looked like *exactly*. Instead, we obtain a probability distribution over possible inputs.\n",
        "\n",
        "$$ p(x) = \\int \\mathrm{d}z\\, p(x|z) p(z) \\tag{3}$$\n",
        "\n",
        "Here, we have introduced two probability distributions on the right hand side, the liklelihood, $p(x|z)$, and the prior, $p(z)$.\n",
        "\n",
        "The likelihood arises because of the information loss in the compression.\n",
        "\n",
        "$$ x = g_\\psi(f_\\phi(x)) + \\epsilon = g_\\psi(z) + \\epsilon \\tag{4}$$ \n",
        "\n",
        "Ideally, $\\epsilon$, the part of the data that is lost in the compression, is just noise and unimportant for our final data analysis. The form of the likelihood is equal to the distribution of this noise. For example, if the noise is Gaussian (which is often the case for physical data) with covariance $\\Sigma_\\epsilon$, the likelihood is a Gaussian distribution:\n",
        "\n",
        "$$ p_{\\psi}(x|z) = \\mathcal{G}(g_\\psi(z),\\Sigma_\\epsilon) \\tag{5}$$ \n",
        "\n",
        "**Optional Question 1**   \n",
        "Starting from $p(x|z) = \\int \\mathrm{d}\\epsilon\\, p(x,\\epsilon|z)$ can you show that the likelihood follows the same distribution as $\\epsilon$?\n",
        "\n",
        "Solution:\n",
        "$p(x|z) = \\int \\mathrm{d}\\epsilon\\, p(x,\\epsilon|z) = \\int \\mathrm{d}\\epsilon\\, p(x|z,\\epsilon) p(\\epsilon) = \\int \\mathrm{d}\\epsilon\\, \\delta_D(x-g(z)-\\epsilon) p(\\epsilon) = p(x-g(z))$ \n",
        "\n",
        "The prior, $p(z)$, is the average distribution of the encoded data.\n",
        "\n",
        "$$ p(z) = \\int \\mathcal{d}x\\, p(z|x) p(x) \\tag{6}$$\n",
        "\n",
        "In a VAE, we want the prior distribution to have closed form and to be easy to sample from (for artificial data generation). We have the freedom to choose the prior distribution as a constraint. The network training will ensure it is obeyed. A common choice is a normal distribution\n",
        "\n",
        "$$ p(z) = \\mathcal{N}(0,1) \\tag{7} $$\n",
        "\n",
        "---\n",
        "\n",
        "### VAE training & Evidence Lower BOund (ELBO)\n",
        "\n",
        "To train the Variational Auto-Encoder we maximize the average log probability, $\\log p(x)$, or, as we will see now, a lower bound to this quantity.\n",
        "\n",
        "Equation (3) involves solving a fairly high dimensional integral, which is a computationally expensive and sometimes infeasible operation. This integral has to be solved not only once, but in each training step. Variational Autoencoders  solve this integral approximately by using a variational ansatz for the posterior distribution, the approximate posterior $q_\\phi(z|x)$. This distribution approximates the true posterior p(z|x) and is parameterized by the encoder parameters $\\phi$. \n",
        "\n",
        "The classic choice for the variational posterior is a multivariate Gaussian in the mean field approxiation (mean field meaning no off-diagonal terms in the covariance)\n",
        "\n",
        "$$ q_\\phi(z|x) = \\mathcal{G}(\\mu,\\sigma_i) \\tag{7} $$\n",
        "\n",
        "where the mean, $\\mu$, and variance, $\\sigma$, are determined by the encoder network. \n",
        "\n",
        "$$(\\mu, \\sigma) = f_\\phi(x) \\tag{8} $$\n",
        "\n",
        "As we saw in the lecture, the variational ansatz allows us to formulate a lower bound to $\\log p(x)$, the Evidence Lower BOund.\n",
        "\n",
        "$$ \\log p(x) >= \\int \\mathrm{d}z\\, q_\\phi(z|x) \\log{p_\\psi(x|z)} - \\int \\mathrm{d}z\\, q_\\phi(z|x) \\log{\\frac{q_\\phi(z|x)}{p(z)}} = ELBO \\tag{8}$$\n",
        "\n",
        "$$ \\mathcal{L}_{VAE}(\\phi,\\psi) = -ELBO \\tag{9}$$\n",
        "\n",
        "The ELBO consists of two terms. The first term measures the expectation value of the likelihood over the posterior. Maximizing this term encourages high quality reconstructions (similar to the autoencoder). The second term is the KL-Divergence (a distance measure) between the variational posterior and the prior. This term acts as a regularizer. It encourages posterior distributions which are similar to the prior.\n",
        "\n",
        "In VAE training the first term is evaluated stochastically, meaning that the expectation value is evaluated approximately by averaging over a number of samples from $q_\\phi(z|x)$. The second term can be either evaluated analytically (the KL divergence between to Gaussian distributions can be calculated) or stochastically. \n",
        "\n",
        "---\n",
        "\n",
        "### Reparametrization Trick\n",
        "\n",
        "Minimizing Eq. (9) requires taking gradients with respect to $\\phi$ and $\\psi$. But how do we take the gradient through an expectation value?  \n",
        "\n",
        "We use what is called the reparametrization trick. Instead of sampling from the posterior $q_\\phi(z|x)$ we sample from the parameter-independent normal distribution\n",
        "\n",
        "$$ \\zeta âˆ¼ \\mathcal{N}(0,1) \\tag{10}$$\n",
        "\n",
        "and use the $z=\\zeta*\\sigma_\\phi+\\mu_\\phi$, an operation which is trivially differentiably, to obtain our samples.\n",
        "\n",
        "Pytorch will perform the reparametrization trick for us under the hood, so that we don't have to code it explicitly.\n"
      ],
      "metadata": {
        "id": "3vg7JmIrF8PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "In this coding exercise we will be working with a galaxy spectra sample from the SDSS-BOSS DR 16 release. The spectra have been de-redshifted to the restframe and their magnitude has been standardized to a distance correspondingg to $z_\\lambda=0.01$. They have further been downsampled to 1000 pixels, denoised and inpainted where masks were present.\n",
        "\n",
        "(I can tell you more about the data cuts and preprocessing if you are interested, but it is not relevant for this task.)\n",
        "\n",
        "Despite being relatively high-dimensional ($d=1000$), galaxy specrtra actually reside on a lower dimensional manifold since we can compress them to much smaller dimensionality without sacrificing much reconstruction quality. \n",
        "\n",
        "This property makes them a very suitable data type for VAEs. \n",
        "\n",
        "(The same applies to image data, but images datasets are computationally more expensive to train on and they need more complicated nework architectures - things that we don't want to worry about for this exercise.)"
      ],
      "metadata": {
        "id": "yDLVu9iy4BFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SDSS_DR16(Dataset):\n",
        "    \"\"\"De-redshifted and downsampled spectra from SDSS-BOSS DR16\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir='/global/cscratch1/sd/vboehm/Datasets/sdss/by_model/', transform=True, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory of data file\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        if train:\n",
        "            self.data = pickle.load(open(os.path.join(root_dir,'SDSS_DR16_preprocessed_train.pkl'),'rb'))\n",
        "        else:\n",
        "            self.data = pickle.load(open(os.path.join(root_dir,'SDSS_DR16_preprocessed_test.pkl'),'rb'))\n",
        "\n",
        "        del self.data['mean']\n",
        "        del self.data['std']\n",
        "        del self.data['SN']\n",
        "        del self.data['spec']\n",
        "\n",
        "        self.keys      = list(self.data.keys())\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['features'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if self.transform:\n",
        "            sample = {key: torch.as_tensor(self.data[key][idx]) for key in self.keys}\n",
        "        else:\n",
        "            sample = {key: self.data[key][idx] for key in self.keys}\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "def get_data(data, loc, batchsize, valid_batchsize=256):\n",
        "    \n",
        "    if data in dir(datasets):\n",
        "        dataset = getattr(datasets,data)\n",
        "    \n",
        "        training_data = dataset(root=loc,train=True,download=True,transform=ToTensor())\n",
        "\n",
        "        valid_data    = dataset(root=loc,train=False,download=True,transform=ToTensor())\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    if batchsize==-1:\n",
        "        batchsize= training_data.__len__()\n",
        "    if valid_batchsize==-1:\n",
        "        valid_batchsize= valid_data.__len__()\n",
        "    train_dataloader = DataLoader(training_data, batch_size=batchsize, shuffle=True)\n",
        "    valid_dataloader = DataLoader(valid_data, batch_size=valid_batchsize, shuffle=True)\n",
        "    \n",
        "    return train_dataloader, valid_dataloader"
      ],
      "metadata": {
        "id": "WhaWK_EWF7R_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}