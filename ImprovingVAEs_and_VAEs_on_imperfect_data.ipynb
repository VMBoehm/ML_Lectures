{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VMBoehm/ML_Lectures/blob/main/ImprovingVAEs_and_VAEs_on_imperfect_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvu91ZyEFtBQ"
      },
      "source": [
        "# **Probabilistic denoising and inpainting of real data with VAEs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vg7JmIrF8PD"
      },
      "source": [
        "by Vanessa Boehm (UC Berkeley and LBNL)   \n",
        "Feb 27 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywKRq_TD5B_Q"
      },
      "source": [
        "In this exercise session, we will start out by improving our VAE model with ome of the techniques we discussed in today's lecture.\n",
        "In the second part we will use the improved VAE model to reconstruct corrupted data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNZMyGZEuMJY"
      },
      "source": [
        "## **Part 1: Enhancing the VAE with a flow prior and $\\mathbf{\\beta}$-VAE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-VDWXPJ5B_b"
      },
      "source": [
        "In yesterday's exercise we saw how the VAE is struggling to simultaneousy minimize both the KL and the likelihood term in the ELBO. A low KL divergence is often associated with a lower reconstruction quality and a high reconstruction quality often comes at the cost of a higher KL-divergence and hence lower sample quality. \n",
        "We can improve on this situation with the help of a probabilistic model that we introduced in today's lecture: A normalizing flow. \n",
        "\n",
        "Recall that a normalizing flow is a non-linear bijective mapping, $z{=}h_\\varphi(u)$, parameterized by $\\varphi$, that has a computationally tractable Jacobian $\\frac{\\partial h_\\varphi(u)}{\\partial u}$. We can use this mapping to approximate a non-Gaussian probabiliy distribution $p(z)$,\n",
        "\n",
        "$$ p(z) \\approx \\log p_\\varphi(z)=\\log \\mathcal{p}_t(h^{-1}(z)) + \\log \\det \\left[\\frac{\\partial h^{-1}_\\varphi(z)}{\\partial z}\\right], \\tag{1}$$\n",
        "\n",
        "where $\\mathcal{p}_t(h^{-1}(z))$ is some tractable distribution, typically a standard normal distribution.\n",
        "\n",
        "Our vanilla VAE used a normal distribution as a prior. With the normalizing flow we can give the prior more flexibility.\n",
        "\n",
        "$$ \\mathrm{ELBO} = \\int \\mathrm{d}z\\, q_\\phi(z|x) \\log{p_\\psi(x|z)} - \\int \\mathrm{d}z\\, q_\\phi(z|x) \\log{\\frac{q_\\phi(z|x)}{p_\\varphi(z)}}, \\tag{2}$$\n",
        "\n",
        "Alternatively, we can change variables and write the ELBO in u-space (u becomes the new latent variable), where the prior remains Gaussian, but the encoder and decoder get complemented with the normalizing flow, \n",
        "\n",
        "$$f_\\psi(z) \\rightarrow f_\\psi(h_\\varphi(u))$$\n",
        "\n",
        "$$g_\\phi(z) \\rightarrow g_\\phi(h^{-1}_\\varphi(u))$$\n",
        "\n",
        "$$ \\mathrm{ELBO} = \\int \\mathrm{d}u\\,  q_\\phi(h_\\varphi^{-1}(u)|x)  \\log{p_\\psi(x|h_\\varphi^{-1}(u))} - \\int \\mathrm{d}u\\, q_\\phi(h_\\varphi^{-1}(u)|x) \\log{\\frac{q_\\phi(h_\\varphi^{-1}(u)|x)}{p_t(u)}}. \\tag{3}$$\n",
        "\n",
        "Both formulations are equivalent.\n",
        "The normalizing flow makes it easier for the model to minimize both terms in the ELBO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uziIHG0q5B_i"
      },
      "source": [
        "#### We start by defining our normalizing flow\n",
        "I have implemented a realNVP. A realNVP is made of several layers. In each layer a subset of the variables is transformed with an affine transform and the remaining variables stay the same. This split makes the computation of the log determinant computationally tracable.\n",
        "\n",
        "Assume we are in the first layer, $l=0$, and we have split the input data into $x = [x^{l=0}_{1:k}, x^{l=0}_{k:d}]$.  The transformation is,\n",
        "\n",
        "$$ x^{l=1}_{1:k}= \\exp(s(x^{l=0}_{k:d}))\\cdot x^{l=0}_{1:k}  + t(x^{l=0}_{k:d})$$\n",
        "\n",
        "$$ x^{l=1}_{k:d}= x^{l=0}_{k:d},$$\n",
        "\n",
        "where $s$ and $t$ are neural networks.\n",
        "\n",
        "Which variables we transform in each layer is a random choice (it could be a trainable parameter!).\n",
        "\n",
        "You don't need to change anything in the RealNVP code below and you can use it as a black box, if you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iquM2NAFde_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.distributions import Normal as Normal\n",
        "from torch.distributions import MultivariateNormal as MultivariateNormal "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RealNVP**"
      ],
      "metadata": {
        "id": "sF0sl2KdiB7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dRVbQRlLbUv"
      },
      "outputs": [],
      "source": [
        "# One layer = a single RealNVP transform\n",
        "class RealNVP_Transform(nn.Module):\n",
        "    def __init__(self, binary_mask):\n",
        "        \"\"\"\n",
        "        binary mask: boolean array of length LATENT_SIZE; the  binary mask determines which part of the data\n",
        "        is transformed and which part stays the same\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size         = len(binary_mask)\n",
        "        self.mask         =  binary_mask[None,:].float()                  \n",
        "        self.inverse_mask = (~binary_mask[None,:]).float()\n",
        "        \n",
        "        #registering the network parameters\n",
        "        self.fc1_s        = torch.nn.Linear(self.size,self.size)\n",
        "        self.fc2_s        = torch.nn.Linear(self.size,self.size)\n",
        "        self.fc1_b        = torch.nn.Linear(self.size,self.size)\n",
        "        self.fc2_b        = torch.nn.Linear(self.size,self.size)\n",
        "\n",
        "\n",
        "    # network for computing s\n",
        "    def network_s(self,x):\n",
        "        x = self.fc1_s(x)\n",
        "        x = torch.nn.LeakyReLU()(x)\n",
        "        x = self.fc2_s(x)\n",
        "        x = torch.clip(x,-5,5)\n",
        "        x = x*self.inverse_mask\n",
        "        return x\n",
        "\n",
        "    # network for computing t\n",
        "    def network_t(self,x):\n",
        "        x = self.fc1_b(x)\n",
        "        x = torch.nn.LeakyReLU()(x)\n",
        "        x = self.fc1_b(x)\n",
        "        x = x*self.inverse_mask\n",
        "        return x\n",
        "\n",
        "    # a forward pass (applies the affine trasnsform to one part of the data and leaves the other part immuted)\n",
        "    def forward(self, u):\n",
        "        masked_input = u*self.mask\n",
        "        input        = u*self.inverse_mask\n",
        "        s            = self.network_s(masked_input)\n",
        "        t            = self.network_t(masked_input)\n",
        "        z            = torch.exp(s)*input+t\n",
        "        log_detJ     = torch.sum(s, axis=-1)\n",
        "        z            = self.inverse_mask*z+masked_input\n",
        "        return z, log_detJ\n",
        "\n",
        "    # inverse of the forward pass\n",
        "    def inverse(self,z):\n",
        "        masked_input = z*self.mask\n",
        "        input        = z*self.inverse_mask\n",
        "        s            = self.network_s(masked_input)\n",
        "        t            = self.network_t(masked_input)\n",
        "        u            = (input-t)/torch.exp(s)\n",
        "        log_detJ_inv = -torch.sum(s,axis=-1)\n",
        "        u            = self.inverse_mask*u+masked_input\n",
        "        return u, log_detJ_inv\n",
        "\n",
        "# the full RealNVP Model\n",
        "class FlowModel(nn.Module):\n",
        "    def __init__(self, layers, dim):\n",
        "        \"\"\"\n",
        "        layers: list of RealNVP_Transform instances; transformation layers\n",
        "        dim   : int; dimensionality of the input data\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim      = dim\n",
        "        self.layers   = nn.ModuleList(layers)\n",
        "        self.depth    = len(self.layers)\n",
        "        self.p_u      = MultivariateNormal(torch.zeros(self.dim), torch.eye(self.dim))\n",
        "\n",
        "    # foward pass\n",
        "    def forward(self,u):\n",
        "        for ii, layer in enumerate(self.layers):\n",
        "          if ii ==0:\n",
        "            z, det = self.layers[ii].forward(u)\n",
        "          else:\n",
        "            z, det_ = self.layers[ii].forward(z)\n",
        "            det+=det_\n",
        "        return z, det\n",
        "\n",
        "    #inverse pass\n",
        "    def inverse(self,z):\n",
        "        for ii in np.arange(self.depth-1,-1,-1):\n",
        "          if ii ==(self.depth-1):\n",
        "            u, inv_det = self.layers[ii].inverse(z)\n",
        "          else:\n",
        "            u, inv_det_ = self.layers[ii].inverse(u)\n",
        "            inv_det+=inv_det_\n",
        "        return u, inv_det\n",
        "\n",
        "    # computes the log prob, using Eq. (1)\n",
        "    def log_prob(self,z):\n",
        "        shape        = z.shape\n",
        "        z            = z.reshape(-1,self.dim)\n",
        "        u, inv_det   = self.inverse(z)\n",
        "        log_prob     = self.p_u.log_prob(u)\n",
        "        log_prob+=inv_det\n",
        "        return log_prob.reshape(shape[:-1])\n",
        "\n",
        "    # sample from p(z) by transforming samples from p(u)\n",
        "    def rsample(self, N_samples):\n",
        "        u        = self.p_u.rsample(N_samples)\n",
        "        z        = self.forward(u)[0]\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Ri65Za5B_3"
      },
      "source": [
        "#### Here's an example for how you would built a RealNVP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0tr-_lR5B_7"
      },
      "outputs": [],
      "source": [
        "# set number of layers and dimensionality of data\n",
        "NLAYERS = 4\n",
        "SIZE    = 4\n",
        "\n",
        "# create random masks, we mask half of the data in each layer\n",
        "if SIZE%2==0:\n",
        "  mask    = np.append(np.zeros(SIZE//2),np.ones(SIZE//2))\n",
        "else:\n",
        "  mask    = np.append(np.zeros(SIZE//2+1),np.ones(SIZE//2))\n",
        "masks= []\n",
        "for ii in range(NLAYERS):\n",
        "  np.random.shuffle(mask)\n",
        "  masks.append(torch.as_tensor(mask).bool())\n",
        "    \n",
        "#initialize the layers\n",
        "layers  = [RealNVP_Transform(masks[ii]) for ii in range(NLAYERS)]\n",
        "#initialize the FlowModel\n",
        "FM      = FlowModel(layers, SIZE)\n",
        "\n",
        "# evaluate the log probability of same data\n",
        "FM.log_prob(torch.as_tensor(np.random.randn(10,4)).float())\n",
        "\n",
        "#test bijectivity\n",
        "test = torch.ones((1,SIZE))\n",
        "print(test)\n",
        "print(FM.forward(FM.inverse(test)[0])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHVFQfgq5B__"
      },
      "source": [
        "With this we can go back to yesterday's VAE model (I just copy pasted the code below). Can you exchange the prior by a FlowPrior?\n",
        "As before, I'll be giving you more detailed instructions in the code, marked by #TASK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMnt57iDXqIJ"
      },
      "outputs": [],
      "source": [
        "INPUT_SIZE      = 1000\n",
        "# Yes, I have changed the latent size!\n",
        "LATENT_SIZE     = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RauugG-yVWQu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q00sc0Ds5CAI"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhaWK_EWF7R_"
      },
      "outputs": [],
      "source": [
        "class SDSS_DR16(Dataset):\n",
        "    \"\"\"De-redshifted and downsampled spectra from SDSS-BOSS DR16\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir='drive/MyDrive/ML_lecture_data/', transform=True, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory of data file\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        if train:\n",
        "            self.data = np.load(open(os.path.join(root_dir,'DR16_denoised_inpainted_train.npy'),'rb'),allow_pickle=True)\n",
        "        else:\n",
        "            self.data = np.load(open(os.path.join(root_dir,'DR16_denoised_inpainted_test.npy'),'rb'),allow_pickle=True)\n",
        "        self.data = torch.as_tensor(self.data)\n",
        "        self.mean = torch.mean(self.data)\n",
        "        self.std  = torch.std(self.data)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample = (self.data[idx]-self.mean)/self.std\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "training_data = SDSS_DR16(train=True)\n",
        "test_data     = SDSS_DR16(train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKMNpHaeb93k"
      },
      "outputs": [],
      "source": [
        "class VAEEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, seed=853):\n",
        "        super(VAEEncoder, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE,50)\n",
        "        self.fc2 = nn.Linear(50,LATENT_SIZE*2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x      = torch.nn.LeakyReLU()(self.fc1(x))\n",
        "        x      = self.fc2(x)\n",
        "        mu,std = torch.split(x, LATENT_SIZE,dim=-1)\n",
        "        std    = torch.exp(std) + 1e-8\n",
        "        return mu, std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-_V5PYNMNSy"
      },
      "outputs": [],
      "source": [
        "class VAEDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, seed=620):\n",
        "        super(VAEDecoder, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(LATENT_SIZE,50)\n",
        "        self.fc2 = nn.Linear(50,INPUT_SIZE)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = torch.nn.LeakyReLU()(self.fc1(z))\n",
        "        x = self.fc2(z)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK: Add a FlowModel as prior to the VAE, use 8 layers and set the random seed to 9303 before you create the masks\n",
        "#HINT: Use this cell to initialize the model\n",
        "NLAYERS = ??\n",
        "SIZE    = ??\n",
        "\n",
        "# create random masks, we mask half of the data in each layer\n",
        "if SIZE%2==0:\n",
        "  mask    = np.append(np.zeros(SIZE//2),np.ones(SIZE//2))\n",
        "else:\n",
        "  mask    = np.append(np.zeros(SIZE//2+1),np.ones(SIZE//2))\n",
        "masks= []\n",
        "np.random.randint(??)\n",
        "for ii in range(NLAYERS):\n",
        "  np.random.shuffle(mask)\n",
        "  masks.append(torch.as_tensor(mask).bool())\n",
        "  print(masks[-1])\n",
        "\n",
        "#initialize the layers\n",
        "layers  = ??"
      ],
      "metadata": {
        "id": "jy-yPe3u6c29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IblX0IgkbcMI"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, sample_size, sigma):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = VAEEncoder()\n",
        "        self.decoder = VAEDecoder()\n",
        "        self.sample_size = sample_size\n",
        "        self.sigma       = sigma\n",
        "        #TASK: insert new prior\n",
        "        self.prior       = ??\n",
        "\n",
        "    def change_sample_size(self,sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        return True\n",
        "\n",
        "    def get_q(self,x):\n",
        "        mu, std = self.encoder(x)\n",
        "        self.q  = Normal(mu, std)\n",
        "        return True\n",
        "\n",
        "    def sample_q(self):\n",
        "        z_sample = self.q.rsample(torch.Size([self.sample_size]))\n",
        "        return z_sample\n",
        "\n",
        "    def get_log_likelihood(self,recons,x):\n",
        "        ll    = Normal(x[None,:,:], self.sigma)\n",
        "        log_p = ll.log_prob(recons)\n",
        "        log_p = torch.sum(log_p,dim=-1)\n",
        "        return log_p\n",
        "\n",
        "    def get_avg_log_likelihood(self,recons,x):\n",
        "        log_p = self.get_log_likelihood(recons,x)\n",
        "        return torch.mean(log_p,dim=0)\n",
        "\n",
        "    def stochastic_kl_divergence(self,z_sample):\n",
        "        #TASK: fix the shape error\n",
        "        #HINT: the output shape of the FlowModel is different from the output shape of the prior we use previously, because the FlowModel uses a multivariate Gaussian as base ditribution\n",
        "        return torch.mean(torch.sum(self.q.log_prob(z_sample),dim=-1)-self.prior.log_prob(z_sample), dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.get_q(x)\n",
        "        samples = self.sample_q()\n",
        "        recons  = self.decoder(samples)\n",
        "        log_likelihood = self.get_avg_log_likelihood(recons,x)\n",
        "        kl      = self.stochastic_kl_divergence(samples)\n",
        "        return log_likelihood, kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DYaUij-WkK3"
      },
      "outputs": [],
      "source": [
        "BATCHSIZE       = 128\n",
        "BATCHSIZE_TEST  = 128\n",
        "LEARNING_RATE   = 1e-3\n",
        "\n",
        "#TASK: Initialize the VAE. Set sigma to 0.1 and sample size to 16\n",
        "VAE       = ??\n",
        "\n",
        "optimizer = torch.optim.Adam(VAE.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# Dataloaders\n",
        "train_dataloader = DataLoader(training_data, batch_size=BATCHSIZE, shuffle=True)\n",
        "test_dataloader  = DataLoader(test_data, batch_size=BATCHSIZE_TEST, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k32pomGB5CAa"
      },
      "source": [
        "Let's check if the additional flow parameters were registered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCj2DIpVu7xk"
      },
      "outputs": [],
      "source": [
        "for name, param in VAE.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zw8MYTJWW4N8"
      },
      "outputs": [],
      "source": [
        "def negative_ELBO(avg_log_likelihood,kl):\n",
        "\n",
        "    negative_ELBO = - torch.mean(avg_log_likelihood-kl)\n",
        "\n",
        "    return negative_ELBO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yjd0yE4WrKc"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    losses = []\n",
        "    for batch, X in enumerate(dataloader):\n",
        "        log_likelihood, kl = model(X)\n",
        "        loss = loss_fn(log_likelihood,kl)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            losses.append(loss)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, nllh, kl_ = 0, 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            log_likelihood, kl = model(X)\n",
        "            test_loss += loss_fn(log_likelihood,kl).item()\n",
        "            nllh += -np.mean(log_likelihood.cpu().numpy())\n",
        "            kl_ += np.mean(kl.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    kl_ /= num_batches\n",
        "    nllh /= num_batches\n",
        "\n",
        "    print(f\" Avg test loss      : {test_loss:>8f}\")\n",
        "    print(f\" Avg KL             : {kl_:>8f}\")\n",
        "    print(f\" Avg negative log likelihood : {nllh:>8f} \\n\")\n",
        "\n",
        "    return test_loss, kl_, nllh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Udr9UnLXmJf"
      },
      "outputs": [],
      "source": [
        "#TASK: What do you observe in the training? How do the results compare to yesterday? \n",
        "#TASK: What happens when you change sigma?\n",
        "#HINT: You don't need to plot all metrics, just look at the ELBO, likleihood and KL divergence on the test set and compare them to the numbers you observed yesterday\n",
        "EPOCHS     = 10\n",
        "train_loss = []\n",
        "test_loss  = []\n",
        "beta_init  = 100\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss.append(train_loop(train_dataloader, VAE, negative_ELBO, optimizer))\n",
        "    test_loss.append(test_loop(test_dataloader, VAE, negative_ELBO))\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7syew_eAXpfj"
      },
      "outputs": [],
      "source": [
        "length = len(np.asarray(train_loss).flatten())\n",
        "test_loss = np.asarray(test_loss)\n",
        "plt.figure()\n",
        "plt.plot(np.linspace(0,length*100,length), np.asarray(train_loss).flatten(),label='training set')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,0],label='test set loss')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,1],label='test set kl')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,2],label='test set neg log likelihood')\n",
        "plt.xlabel('training step')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(-1500,1000)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOjexoI5CAm"
      },
      "source": [
        "### **$\\beta$-VAE**\n",
        "Since the results are still not 100% satisfying, we will introduce another trick: artificial upweighting of the KL term."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAE       = VariationalAutoencoder(sample_size=16,sigma=0.1)\n",
        "\n",
        "optimizer  = torch.optim.Adam(VAE.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "RDminj0lS3oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXyqGa4h5CAo"
      },
      "outputs": [],
      "source": [
        "#TASK: add a parameter $\\beta$, $\\beta>=0$, to the ELBO.\n",
        "#HINT: the default value should be 1\n",
        "def negative_ELBO(avg_log_likelihood,kl, ??):\n",
        "\n",
        "    negative_ELBO = ??\n",
        "\n",
        "    return negative_ELBO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08HYXh3e-yRJ"
      },
      "outputs": [],
      "source": [
        "#TASK: add beta\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, ??):\n",
        "    size = len(dataloader.dataset)\n",
        "    losses = []\n",
        "    for batch, X in enumerate(dataloader):\n",
        "        log_likelihood, kl = model(X)\n",
        "        loss = ??\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0, norm_type=2)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            losses.append(loss)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "#TASK: add beta\n",
        "def test_loop(dataloader, model, loss_fn, ??):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, nllh, kl_ = 0, 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            log_likelihood, kl = model(X)\n",
        "            test_loss += ??\n",
        "            nllh += -np.mean(log_likelihood.cpu().numpy())\n",
        "            kl_ += np.mean(kl.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    kl_ /= num_batches\n",
        "    nllh /= num_batches\n",
        "\n",
        "    print(f\" Avg test loss      : {test_loss:>8f}\")\n",
        "    print(f\" Avg KL             : {kl_:>8f}\")\n",
        "    print(f\" Avg negative log likelihood : {nllh:>8f} \\n\")\n",
        "\n",
        "    return test_loss, kl_, nllh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3SJCZmI5CAp"
      },
      "outputs": [],
      "source": [
        "#TASK: Train the beta-VAE with different values of beta. (Suggested: [10,100,500])\n",
        "#TASK: try annealing beta during training\n",
        "#TASK: which of these procedures works best? (look at all metrics:  1) recon error, 2) visual sample quality, 3) match between prior and average posterior, code is given below)\n",
        "#HINT: beta must always be 1 during evaluation, otherwise we are not estimating an Evidence Lower BOund anymore!\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "train_loss = []\n",
        "test_loss  = []\n",
        "beta_init  = ??\n",
        "for t in range(EPOCHS):\n",
        "    beta = ??\n",
        "    print('beta:', beta)\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss.append(train_loop(train_dataloader, VAE, negative_ELBO, optimizer,beta))\n",
        "    test_loss.append(test_loop(test_dataloader, VAE, negative_ELBO, beta=1))\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: save weights\n",
        "#torch.save(VAE.state_dict(), 'drive/MyDrive/ML_lecture_models/ImprovedVAE_model_weights_beta100.pth')"
      ],
      "metadata": {
        "id": "5f0ULXnrQ25A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr58J2VF5CAs"
      },
      "outputs": [],
      "source": [
        "test_loss=np.asarray(test_loss)\n",
        "length = len(np.asarray(train_loss).flatten())\n",
        "plt.figure()\n",
        "plt.plot(np.linspace(0,length*100,length), np.asarray(train_loss).flatten(),label='training set')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,0],label='test set loss')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,1],label='test set kl')\n",
        "plt.plot(np.linspace(100,(length)*100,len(test_loss)),test_loss[:,2],label='test set neg log likelihood')\n",
        "plt.xlabel('training step')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(-1500,1000)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvdilGwgXmKU"
      },
      "outputs": [],
      "source": [
        "#TASK: compare this recon error to what you plotted for the vanilla VAE yesterday\n",
        "wlmin, wlmax      = (3388,8318)\n",
        "fixed_num_bins    = 1000\n",
        "wl_range          = (np.log10(wlmin),np.log10(wlmax))\n",
        "wl                = np.logspace(wl_range[0],wl_range[1],fixed_num_bins) \n",
        " \n",
        "avg_loss  = 0\n",
        "VAE.eval()\n",
        "with torch.no_grad():\n",
        "    for X in test_dataloader:\n",
        "        pred = VAE.decoder(VAE.encoder(X)[0])\n",
        "        avg_loss+=np.mean((pred.cpu().numpy()-X.cpu().numpy())**2,axis=0)/(len(test_data)//BATCHSIZE_TEST)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(wl, np.sqrt(avg_loss))\n",
        "plt.ylabel('average reconstruction error')\n",
        "plt.xlabel('wavelength [Ångströms]')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zbCDqW93Vwp"
      },
      "outputs": [],
      "source": [
        "#TASK: make a corner plot of posterior samples. Does the average posterior match the prior?\n",
        "#TASK Can you change the code to compare the samples in u-space? (Both prior and posterior should be Gaussian in that space)\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "VAE.eval()\n",
        "with torch.no_grad():\n",
        "    for ii, X in enumerate(test_dataloader):\n",
        "        VAE.get_q(X)\n",
        "        prior_sample = ??\n",
        "        sample       = ??\n",
        "        if ii==0:\n",
        "          samples       = sample\n",
        "          prior_samples = prior_sample\n",
        "        else:\n",
        "          samples       = np.vstack([samples, sample])\n",
        "          prior_samples = np.vstack([prior_samples, prior_sample])\n",
        "\n",
        "samples       = np.reshape(samples,[-1, LATENT_SIZE])\n",
        "prior_samples = np.reshape(prior_samples,[-1, LATENT_SIZE])\n",
        "\n",
        "print(samples.shape)\n",
        "print(prior_samples.shape)\n",
        "\n",
        "data1    = pd.DataFrame()\n",
        "data2    = pd.DataFrame()\n",
        "\n",
        "for ii in range(LATENT_SIZE):\n",
        "  data1['dim_%d'%ii] = samples[:,ii]\n",
        "data1['source'] = 'posterior'\n",
        "\n",
        "for ii in range(LATENT_SIZE):\n",
        "  data2['dim_%d'%ii] = prior_samples[:,ii]\n",
        "data2['source'] = 'prior'\n",
        "\n",
        "data = pd.concat([data1,data2]).reset_index(drop=True)\n",
        "\n",
        "sns.pairplot(data,corner=True,kind='scatter', hue='source', plot_kws={'s':2})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhYlgfZLm_iI"
      },
      "outputs": [],
      "source": [
        "# random samples\n",
        "VAE.eval()\n",
        "with torch.no_grad():\n",
        "  samples      = ??\n",
        "  data_samples = ??\n",
        "\n",
        "fig, ax = plt.subplots(4,4, figsize=(20,10), sharex=True)\n",
        "ax = ax.flatten()\n",
        "for ii in range(16):\n",
        "  ax[ii].plot(wl,data_samples[ii], label='artificial data')\n",
        "  if ii in np.arange(12,16):\n",
        "    ax[ii].set_xlabel('wavelength [Ångströms]')\n",
        "  if ii in [0,4,8,12]:\n",
        "    ax[ii].set_ylabel('some standardized flux')\n",
        "  if ii==0:\n",
        "    ax[ii].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRdpM4ua5CA0"
      },
      "source": [
        "## **Part 1b: Testing the variational posterior**\n",
        "How well does the variational posterior $q(z|x)$ approximate the real posterior $p(z|x)$? Let's have a look!\n",
        "We can sample from $\\log p(z|x)$ because we know $\\log p(z|x)$ up to a constant:\n",
        "$$\\log p(z|x) = \\log p(x|z)+ \\log p(z)- const$$\n",
        "Sampling becomes difficult in high dimensions, but since we have reduced the dimensionality to 4, we can try and sample the distribution with a simple Metropolis-Hastings Algorithm.\n",
        "\n",
        "You don't need to know how a Metropolis-Hastings algorithm works for this exercise, I'll provide code below. But it's worth pointing out that the algorithm only uses ratios between probabilities of different points, which means that constants cancel out.\n",
        "\n",
        "You can work with the model you trained above or you can use pretrained weights.\n",
        "\n",
        "To load the pre-trained weights, you need to first [download them](https://drive.google.com/file/d/1KPle5I2q177uO5a-wF-yM_LUSN7cxpdf/view?usp=sharing) and put them in your Google Drive folder. Then uncomment the next two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8z-ptYkFZgU"
      },
      "outputs": [],
      "source": [
        "#VAE.load_state_dict(torch.load('drive/MyDrive/ML_lecture_models/ImprovedVAE_model_weights_beta100.pth'))\n",
        "#VAE.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metropolis-Hastings Algorithm"
      ],
      "metadata": {
        "id": "XW5qRusbpQ0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9MbbKQT5CA4"
      },
      "outputs": [],
      "source": [
        "# This is a Metropolis Hastings algorithm. \n",
        "class MetropolisHastings():\n",
        "    def __init__(self,dist,dim,std,mu,init,burn_in=1000):\n",
        "        \n",
        "        self.dim = dim\n",
        "        self.std = std\n",
        "        self.mu  = mu\n",
        "        self.current_state = init\n",
        "        self.dist          = dist\n",
        "        self.current_log_prob = dist(init)\n",
        "        self.burn_in       = burn_in\n",
        "\n",
        "        \n",
        "    def suggest_step(self):\n",
        "        step = np.random.randn(self.dim)\n",
        "        step = self.std*step+self.mu\n",
        "        return step.astype(np.float32)\n",
        "    \n",
        "    def update_state(self):\n",
        "        candidate = self.current_state+self.suggest_step()\n",
        "        cand_log_prob = self.dist(candidate)\n",
        "        ratio     = np.exp(cand_log_prob-self.current_log_prob)\n",
        "        A         = min(ratio,1)\n",
        "        dice      = np.random.rand()\n",
        "        accepted  = 0\n",
        "        if dice<= A:\n",
        "            self.current_state = candidate\n",
        "            self.current_log_prob = cand_log_prob\n",
        "            accepted = 1\n",
        "        return self.current_state, self.current_log_prob, accepted\n",
        "    \n",
        "    def sample(self,N_samples):\n",
        "        ii = 0\n",
        "        samples   = []\n",
        "        log_probs = []\n",
        "        accepted  = 0\n",
        "        while ii<=N_samples+self.burn_in:\n",
        "            sample, log_prob,acc = self.update_state()\n",
        "            if torch.is_tensor(sample):\n",
        "              sample=np.squeeze(sample.cpu().numpy())\n",
        "            if ii>self.burn_in:\n",
        "              samples.append(sample)\n",
        "              log_probs.append(log_prob)\n",
        "              accepted+=acc\n",
        "            ii+=1\n",
        "        return np.asarray(samples), np.asarray(log_probs), accepted/N_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpNjtS9F5CA6"
      },
      "source": [
        ":Here's an example for how to use the sampling alorithm. \n",
        "I use a multivariate Gaussian as target dsitribution here. We can sample from this distribution without a sampling alorithm and his allows us to evaluate the perfromance of the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRbFT3uI5CA8"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# To initialize the sampler, we need to pass the function that evaluates the log probability of the distribution\n",
        "dist = multivariate_normal(mean=np.asarray([0,0]), cov=np.asarray([[1,-1],[-1,2]]))\n",
        "# the dimenionality of the problem,\n",
        "dim  = 2\n",
        "# the mean and standard deviation of the proposal distribution\n",
        "std  = np.asarray([0.3,0.3])\n",
        "mu   = np.asarray([0.0,0.0])\n",
        "# and the starting point\n",
        "init = np.asarray([0.,0.])\n",
        "\n",
        "MH = MetropolisHastings(dist.logpdf,dim,std,mu,init)\n",
        "\n",
        "#draw 100000 samples\n",
        "samples, log_probs, acc_ratio = MH.sample(10000)\n",
        "#The acceptance ratio should be between 0.6 and 0.8. \n",
        "#Too high could mean that parts of the distribution remain unsampled.\n",
        "#Too low means your sampler is not efficienly exploring\n",
        "#You can change the acc_ratio by changing the std of the proposal distribution\n",
        "print(acc_ratio)\n",
        "\n",
        "#draw correct samples from the dsitribution\n",
        "real_samples       = dist.rvs(size=10000)\n",
        "\n",
        "#compare\n",
        "plt.scatter(samples[:,0],samples[:,1], s=2, label='MCMC')\n",
        "plt.scatter(real_samples[:,0],real_samples[:,1], s=2, label='true dis')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6529MbI5CA_"
      },
      "source": [
        "Let's sample from the variational posterior for a data point x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUFvwefkG0Mr"
      },
      "outputs": [],
      "source": [
        "#TASK: write a function that returns a function that returns log p(z,x). \n",
        "def get_log_joint_prob(VAE, x):\n",
        "    def log_joint_prob(z):\n",
        "        recon = VAE.decoder(z)\n",
        "        llh   = VAE.get_log_likelihood(recon,x)\n",
        "        lpz   = VAE.prior.log_prob(z)\n",
        "        return llh+lpz\n",
        "    return log_joint_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# draw a data sample\n",
        "x = iter(test_dataloader).next()\n",
        "x = x[0:1]\n",
        "\n",
        "plt.plot(wl, np.squeeze(x))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TTCoKJcyVyjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U38_y0nh5CBA"
      },
      "outputs": [],
      "source": [
        "#TASK: initiate a Metropolis-Hastings alorithm for sampling p(z,x)\n",
        "#TASK: draw samples from p(z|x) for the nth entry in the test set\n",
        "#HINT: you might need to adapt the std in order to achieve a good acceptance ratio\n",
        "#HINT: use the encoded value as the starting point, if you see a \"burn in\", try different starting values\n",
        "\n",
        "with torch.no_grad():\n",
        "  log_prob = lambda y: get_log_joint_prob(VAE, x)(y).item()\n",
        "  mu, std  = VAE.encoder(x)\n",
        "  MH       = MetropolisHastings(log_prob,dim=LATENT_SIZE,std=np.ones(LATENT_SIZE,dtype=np.float32)*0.02,mu=np.zeros(LATENT_SIZE, dtype=np.float32),init=np.squeeze(mu))\n",
        "\n",
        "  MCMC_samples, _, acceptance_ratio = MH.sample(10000)\n",
        "\n",
        "print('acceptance ratio: ',acceptance_ratio)\n",
        "\n",
        "#TASK: draw samples from the variational posterior\n",
        "approx_posterior_samples  = std*np.random.randn(10000,LATENT_SIZE)+mu\n",
        "\n",
        "\n",
        "#TASK: plot. What do you find?\n",
        "plt.scatter(approx_posterior_samples[:,0],approx_posterior_samples[:,1], s=2, label='approximate posterior')\n",
        "plt.scatter(MCMC_samples[:,0],MCMC_samples[:,1], s=2, label='real posterior')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#TASK: Plot some of the samples in data space. Do you see a difference between the MCMC samples and the variational inference samples?\n",
        "with torch.no_grad():\n",
        "  post_samples = VAE.decoder(torch.as_tensor(MCMC_samples[:16]).float())\n",
        "  q_samples = VAE.decoder(torch.as_tensor(approx_posterior_samples[:16]).float())\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(2,1, figsize=(8,5), sharex=True)\n",
        "ax = ax.flatten()\n",
        "for ii in range(16):\n",
        "    ax[0].plot(wl,post_samples[ii])\n",
        "    ax[1].plot(wl,q_samples[ii])\n",
        "ax[0].set_title(r'$p(z|x)$')\n",
        "ax[1].set_title(r'$q(z|x)$')\n",
        "ax[1].set_xlabel('wavelength [Ångströms]')\n",
        "ax[0].legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXPK-x_b5CBD"
      },
      "source": [
        "### Take-away message: The variational posterior can be a bad approximation! It is generally not recommended to use it for scientific data analysis! \n",
        "You can use sampling algorithms on p(z,x) or other techniques for a more reliable posterior analysis.\n",
        "Some sampling algorithms use derivatives to explore high dimensional spaces more efficiently (e.g. HMC). They can still perform reasonably well in situations where other alorithms struggle. With pytorch we can take derivative wrt to model inputs, as we will see below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nGHoqhR5CBE"
      },
      "source": [
        "## **Part 2: Reconstructing corrupted data**\n",
        "We will now use some of the techniques we introduced above to denoise and inpaint noisy and masked data. \n",
        "\n",
        "Corrupted data, $\\tilde {x}$ is related to uncorrupted data, $x$, through:\n",
        "\n",
        "$$ \\tilde{x} = M x + n, \\tag{4}$$\n",
        "\n",
        "where $M$ is the mask and $n$ is the noise.\n",
        "\n",
        "For Gaussian noise with covariance $N$, we saw yesterday that this implies a Gaussian likelihood\n",
        "\n",
        "$$ p(\\tilde{x}|x) = \\mathcal{G}(\\tilde{x}|x,N) \\tag{5}$$\n",
        "\n",
        "For reconstruction we are interested in the posterior \n",
        "\n",
        "$$ p(x|\\tilde{x}) \\sim p(x,\\tilde{x}) = p(x|\\tilde{x}) p(x) \\tag{6}$$\n",
        "\n",
        "There are two problems with Eq.(6)\n",
        "\n",
        "1. The posterior is extremely high dimensional (and sampling or analysing it basically impossible)\n",
        "2. We don't know $p(x)$\n",
        "\n",
        "We can solve both of these problems with the help of our VAE model. Our VAE model provides a mapping from the high dimensional data space, $x$, to a low dimensional space $z$. This mapping was trained to minimize the reconstruction error, i.e. we do not loose much information about $x$ by going to $z$. Also, we have access to the prior distribution $p(z)$. With these two ingredients, our forward model can be written as\n",
        "\n",
        "$$ \\tilde{x} = M g(z) + n, \\tag{7}$$\n",
        "\n",
        "where $g$ is the VAE decoder.\n",
        "\n",
        "The likelihood becomes\n",
        "\n",
        "$$ p(\\tilde{x}|z) = \\mathcal{G(\\tilde{x}|g(z),N)}.$$\n",
        "\n",
        "(If the reconstruction error of the VAE is not neligible compared to the noise, the two can be added quadratically).\n",
        "\n",
        "And finally, we can write the posterior in the lower dimensional latent space\n",
        "\n",
        "$$ p(z|\\tilde{x}) \\sim p(z,\\tilde{x}) = p(z|\\tilde{x}) p(z), \\tag{8}$$\n",
        "\n",
        "with the VAE prior $p(z)$.\n",
        "\n",
        "This posterior is low dimensional and we can sample from it or find its maximum (the most likely reconstruction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2hdcRhZ5CBH"
      },
      "source": [
        "Let's start by importing some *real* data.\n",
        "The data files can be found [here](https://drive.google.com/file/d/1D2dcGbCgVuOCU1fpPP5S33mTW2BAFRkX/view?usp=sharing) and should be placed in your google drive.\n",
        "\n",
        "The Dataset method $__getitem__$ now returns a dictionary. The dictionary entries are data['spec'], which is the noisy and masked data, data['mask'] and data['noise'], which are the mask and inverse noise variance, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WHyEu2H5CBJ"
      },
      "outputs": [],
      "source": [
        "class SDSS_DR16_corrupted(Dataset):\n",
        "    \"\"\"De-redshifted and downsampled spectra from SDSS-BOSS DR16, this time with noise and everything\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir='drive/MyDrive/ML_lecture_data/', transform=True, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory of data file\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        if train:\n",
        "            self.data = np.load(open(os.path.join(root_dir,'DR16_train.npy'),'rb'),allow_pickle=True)\n",
        "        else:\n",
        "            self.data = np.load(open(os.path.join(root_dir,'DR16_test.npy'),'rb'),allow_pickle=True)\n",
        "        self.spec = torch.as_tensor(self.data['spec'])\n",
        "        self.mask = torch.as_tensor(self.data['mask'])\n",
        "        self.noise= torch.as_tensor(self.data['noise'])\n",
        "        self.mean = torch.mean(self.spec)\n",
        "        self.std  = torch.std(self.spec)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        sample = {}\n",
        "        sample['spec']  = np.squeeze((self.spec[idx]-self.mean)/self.std).float()\n",
        "        sample['noise'] = np.squeeze(self.noise[idx]*self.std**2).float()\n",
        "        sample['mask']  = np.squeeze(self.mask[idx]).float()\n",
        "        \n",
        "        return sample\n",
        "\n",
        "\n",
        "test_corrupted = SDSS_DR16_corrupted(train=False)\n",
        "\n",
        "test_corrupted_loader  = DataLoader(test_corrupted, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gobanU7M5CBM"
      },
      "outputs": [],
      "source": [
        "#TASK: plot some data samples. Observe how they are noisy? The masked parts have value 0.\n",
        "\n",
        "fig, ax = plt.subplots(4,4, figsize=(20,10), sharex=True)\n",
        "ax = ax.flatten()\n",
        "for ii in range(16):\n",
        "  data_sample = ??\n",
        "  ax[ii].plot(wl,data_sample, label='artificial data')\n",
        "  if ii in np.arange(12,16):\n",
        "    ax[ii].set_xlabel('wavelength [Ångströms]')\n",
        "  if ii in [0,4,8,12]:\n",
        "    ax[ii].set_ylabel('some standardized flux')\n",
        "  if ii==0:\n",
        "    ax[ii].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD3EVlku5CBO"
      },
      "outputs": [],
      "source": [
        "#TASK: write a function that computes the lo likleihood of corrupted data p(\\tilde(x)|z)\n",
        "#HINT: use torch.masked_select\n",
        "#HINT: data['noise'] is the inverse variance 1/\\sigma^2\n",
        "def corrupted_log_likelihood(x,recon,mask,noise):\n",
        "    x     = ??\n",
        "    noise = ??\n",
        "    recon = ??\n",
        "    ll    = Normal(??\n",
        "    log_p = ??\n",
        "    log_p = torch.sum(log_p,dim=-1)\n",
        "    return log_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDRn0fSo5CBP"
      },
      "outputs": [],
      "source": [
        "#TASK: write a function that return a function that returns p(z,\\tilde{x})\n",
        "def get_corrupted_log_joint_prob(VAE, x, mask, noise):\n",
        "    def corrupted_log_joint_prob(z):\n",
        "        recon = ??\n",
        "        llh   = corrupted_log_likelihood(recon,x,mask, noise)\n",
        "        lpz   = ??\n",
        "        return llh+lpz\n",
        "    return corrupted_log_joint_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the most likely reconstruction"
      ],
      "metadata": {
        "id": "PkVJ8nk_uEig"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCaZg4rC5CBR"
      },
      "outputs": [],
      "source": [
        "#TASK: Start from the encoded value f(\\tilde(x)) and maximize p(z,\\tilde(x)) wrt to z\n",
        "\n",
        "#turn off gradients wrt neural network parameters\n",
        "for param in VAE.parameters():\n",
        "  param.requires_grad=False\n",
        "\n",
        "data_sample = next(iter(test_corrupted_loader))\n",
        "#TASK: Load a data sample\n",
        "x           = ??\n",
        "#TASK: get initial value\n",
        "z_init      = ??\n",
        "\n",
        "corrupted_log_joint_prob = get_corrupted_log_joint_prob(VAE, x, data_sample['mask'].bool(), data_sample['noise'])\n",
        "\n",
        "#Tell torch to track he input. This allows us to take the derivative wrt to it\n",
        "z           = torch.autograd.Variable(torch.as_tensor(z_init),requires_grad=True)\n",
        "\n",
        "#\n",
        "optim       = torch.optim.Adam([z],lr=1e-3)\n",
        "\n",
        "losses      = []\n",
        "for ii in range(3000):\n",
        "    ll = ??\n",
        "    optim.zero_grad()\n",
        "    ll.backward()\n",
        "    optim.step()\n",
        "    if ii%100==0:\n",
        "      print(ll.item())\n",
        "    losses.append(ll.item())\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TASK: foward model the result into data space, compare to the foward modeled encoded value\n",
        "plt.plot(wl,np.squeeze(x), label='noisy, masked, input')\n",
        "plt.plot(wl,??,label='most likely reconstruction')\n",
        "plt.plot(wl,??,label='maximum of $q(z)$')\n",
        "plt.xlabel('wavelength [Ångströms]')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KgXTB_du6UK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Posterior Samples"
      ],
      "metadata": {
        "id": "b19UULhGuIbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxRVKNzX5CBT"
      },
      "outputs": [],
      "source": [
        "#TASK: Use the metropolis hastings sampler to sample from the posterior of a data sample ...\n",
        "#TASK: forward model some of the samples -> by how much do they vary?\n",
        "\n",
        "with torch.no_grad():\n",
        "  log_prob = lambda y: ??.item()\n",
        "  mu, _    = VAE.encoder(x)\n",
        "  MH       = MetropolisHastings(??,dim=LATENT_SIZE,std=np.ones(LATENT_SIZE,dtype=np.float32)*0.01,mu=np.zeros(LATENT_SIZE, dtype=np.float32),init=np.squeeze(z.detach()))\n",
        "\n",
        "  MCMC_samples, _, acceptance_ratio = ??\n",
        "\n",
        "print('acceptance ratio: ',acceptance_ratio)\n",
        "\n",
        "with torch.no_grad():\n",
        "  post_samples = ??(torch.as_tensor(MCMC_samples[:16]).float())\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(6,4), sharex=True)\n",
        "for ii in range(16):\n",
        "    ax.plot(wl,post_samples[ii])\n",
        "ax.set_title(r'$p(z|x)$')\n",
        "ax.set_xlabel('wavelength [Ångströms]')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ImprovingVAEs_and_VAEs_on_imperfect_data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}